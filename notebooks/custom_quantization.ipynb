{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom integer quantization for SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import snntorch as snn\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleksa_tum/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([31, 128, 2, 34, 34])\n",
      "Targets shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as torchvision_transforms\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "\n",
    "# Define sensor size for NMNIST dataset\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "\n",
    "# Define transformations\n",
    "# Note: The use of torch.from_numpy is removed as Tonic's transforms handle conversion.\n",
    "transform = tonic.transforms.Compose([\n",
    "    transforms.Denoise(filter_time=10000),\n",
    "    transforms.ToFrame(sensor_size=sensor_size, time_window=10000),\n",
    "    # torchvision.transforms.RandomRotation is not directly applicable to event data.\n",
    "    # If rotation is needed, it should be done on the frames after conversion by ToFrame.\n",
    "])\n",
    "\n",
    "# Load NMNIST datasets without caching\n",
    "trainset = tonic.datasets.NMNIST(save_to='tmp/data', transform=transform, train=True)\n",
    "testset = tonic.datasets.NMNIST(save_to='tmp/data', transform=transform, train=False)\n",
    "\n",
    "# Split trainset into training and validation datasets\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training, validation, and testing\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
    "\n",
    "# Fetch a single batch from the train_loader to inspect the shape\n",
    "data, targets = next(iter(train_loader))\n",
    "print(f\"Data shape: {data.shape}\")  # Example output: torch.Size([batch_size, timesteps, channels, height, width])\n",
    "print(f\"Targets shape: {targets.shape}\")  # Example output: torch.Size([batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # SNN\n",
    "    \"threshold1\": 2.5,\n",
    "    \"threshold2\": 8.0,\n",
    "    \"threshold3\": 4.0,\n",
    "    \"beta\": 0.5,\n",
    "    \"num_steps\": 10,\n",
    "    \n",
    "    # SNN Dense Shape\n",
    "    \"dense1_input\": 2312,\n",
    "    \"num_classes\": 10,\n",
    "\n",
    "    # Hyper Params\n",
    "    \"lr\": 0.007,\n",
    "\n",
    "    # Early Stopping\n",
    "    \"min_delta\": 1e-6,\n",
    "    \"patience_es\": 20,\n",
    "\n",
    "    # Training\n",
    "    \"epochs\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last model trained with FC layers and LIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "  def __init__(self, config, gather_mem_stats = None):\n",
    "    super(SNN, self).__init__()\n",
    "\n",
    "    # Initialize configuration parameters\n",
    "      # LIF\n",
    "    self.thresh1 = config[\"threshold1\"]\n",
    "    self.thresh2 = config[\"threshold2\"]\n",
    "    self.thresh3 = config[\"threshold3\"]\n",
    "    self.beta = config[\"beta\"]\n",
    "    self.num_steps = config[\"num_steps\"]\n",
    "\n",
    "      # Dense Shape\n",
    "    self.dense1_input = config[\"dense1_input\"]\n",
    "    self.num_classes = config[\"num_classes\"]\n",
    "\n",
    "      # Network Layers\n",
    "    self.fc1 = nn.Linear(self.dense1_input, self.dense1_input//4)\n",
    "    self.lif1 = snn.Leaky(beta=self.beta, threshold=self.thresh1)\n",
    "    \n",
    "    self.fc2 = nn.Linear(self.dense1_input//4, self.dense1_input//8)\n",
    "    self.lif2 = snn.Leaky(beta=self.beta, threshold=self.thresh2)\n",
    "    \n",
    "    self.fc3 = nn.Linear(self.dense1_input//8, self.num_classes)\n",
    "    self.lif3 = snn.Leaky(beta=self.beta, threshold=self.thresh3)\n",
    "    \n",
    "    self.flatten = nn.Flatten()\n",
    "\n",
    "    self.gather_mem_stats = gather_mem_stats\n",
    "    if gather_mem_stats is not None:\n",
    "      self.mem1_val = None\n",
    "      self.mem2_val = None\n",
    "      self.mem3_val = None\n",
    "    \n",
    "    \n",
    "    # Forward Pass\n",
    "  def forward(self, inpt):\n",
    "    mem1 = self.lif1.init_leaky()\n",
    "    mem2 = self.lif2.init_leaky()\n",
    "    mem3 = self.lif3.init_leaky()\n",
    "\n",
    "    spike3_rec = []\n",
    "    mem3_rec = []\n",
    "\n",
    "    for step in range(inpt.shape[0]):\n",
    "      #print(inpt[step].shape)\n",
    "      \n",
    "      current_input = inpt[step]\n",
    "      current_input = self.flatten(current_input)\n",
    "      \n",
    "      current1 = self.fc1(current_input)\n",
    "      spike1, mem1 = self.lif1(current1, mem1)\n",
    "\n",
    "      if self.gather_mem_stats is not None:\n",
    "        if self.mem1_val is None:\n",
    "          self.mem1_val = mem1.flatten().clone()\n",
    "        else:\n",
    "          self.mem1_val = torch.cat((self.mem1_val, mem1.flatten()))\n",
    "\n",
    "      current2 = self.fc2(spike1)\n",
    "      spike2, mem2 = self.lif2(current2, mem2)\n",
    "\n",
    "      if self.gather_mem_stats is not None:\n",
    "        if self.mem2_val is None:\n",
    "          self.mem2_val = mem2.flatten().clone()\n",
    "        else:\n",
    "          self.mem2_val = torch.cat((self.mem2_val, mem2.flatten()))\n",
    "\n",
    "      current3 = self.fc3(spike2)\n",
    "      spike3, mem3 = self.lif3(current3, mem3)\n",
    "\n",
    "      if self.gather_mem_stats is not None:\n",
    "        if self.mem3_val is None:\n",
    "          self.mem3_val = mem3.flatten().clone()\n",
    "        else:\n",
    "          self.mem3_val = torch.cat((self.mem3_val, mem3.flatten()))\n",
    "\n",
    "      spike3_rec.append(spike3)\n",
    "      mem3_rec.append(mem3)\n",
    "\n",
    "    return torch.stack(spike3_rec, dim=0), torch.stack(mem3_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantizeFixedPoint(width, frac, X):\n",
    "    step = 2 ** (-frac)\n",
    "    max_val = (2 ** (width - 1)) - 1\n",
    "    min_val = -(2 ** (width - 1))\n",
    "\n",
    "    X_q = torch.round(X / step)\n",
    "    X_q = torch.clamp(X_q, min_val, max_val)\n",
    "    return X_q * step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving quantized weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading model's parameters\n",
    "model_path = 'best_SNN_model.pth'\n",
    "model = SNN(config, False)\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Quantizing weights\n",
    "model.fc2.weight.data = quantizeFixedPoint(8, 7, model.fc2.weight.data)\n",
    "model.fc1.weight.data = quantizeFixedPoint(8, 7, model.fc1.weight.data)\n",
    "model.fc3.weight.data = quantizeFixedPoint(8, 7, model.fc3.weight.data)\n",
    "\n",
    "# Quantizing biases\n",
    "model.fc1.bias.data = quantizeFixedPoint(8, 7, model.fc1.bias.data)\n",
    "model.fc2.bias.data = quantizeFixedPoint(8, 7, model.fc2.bias.data)\n",
    "model.fc3.bias.data = quantizeFixedPoint(8, 7, model.fc3.bias.data)\n",
    "\n",
    "# Convert the tensor to a pandas DataFrame\n",
    "df1 = pd.DataFrame(torch.transpose(model.fc1.weight.data, 0, 1).numpy())\n",
    "df2 = pd.DataFrame(torch.transpose(model.fc2.weight.data, 0, 1).numpy())\n",
    "df3 = pd.DataFrame(torch.transpose(model.fc3.weight.data, 0, 1).numpy())\n",
    "\n",
    "df4 = pd.DataFrame(model.fc1.bias.data.numpy())\n",
    "df5 = pd.DataFrame(model.fc2.bias.data.numpy())\n",
    "df6 = pd.DataFrame(model.fc3.bias.data.numpy())\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df1.to_csv('fc1_weights.csv', index=False, header=False)\n",
    "df2.to_csv('fc2_weights.csv', index=False, header=False)\n",
    "df3.to_csv('fc3_weights.csv', index=False, header=False)\n",
    "\n",
    "df4.to_csv('fc1_bias.csv', index=False, header=False)\n",
    "df5.to_csv('fc2_bias.csv', index=False, header=False)\n",
    "df6.to_csv('fc3_bias.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     52\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[49], line 29\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, test_loader, criterion, device, model_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Iterate over the test data\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, targets \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m     30\u001b[0m         data, targets \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tonic/datasets/nmnist.py:120\u001b[0m, in \u001b[0;36mNMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    118\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index]\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tonic/transforms.py:32\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, events)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(events) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     events \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m events\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tonic/transforms.py:119\u001b[0m, in \u001b[0;36mDenoise.__call__\u001b[0;34m(self, events)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, events):\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdenoise_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_time\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tonic/functional/denoise.py:34\u001b[0m, in \u001b[0;36mdenoise_numpy\u001b[0;34m(events, filter_time)\u001b[0m\n\u001b[1;32m     30\u001b[0m t \u001b[38;5;241m=\u001b[39m event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     31\u001b[0m timestamp_memory[x, y] \u001b[38;5;241m=\u001b[39m t \u001b[38;5;241m+\u001b[39m filter_time\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     (x \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m timestamp_memory[x \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, y] \u001b[38;5;241m>\u001b[39m t)\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (x \u001b[38;5;241m<\u001b[39m width \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m timestamp_memory[x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, y] \u001b[38;5;241m>\u001b[39m t)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (y \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m timestamp_memory[x, y \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m t)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (y \u001b[38;5;241m<\u001b[39m height \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m timestamp_memory[x, y \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m t)\n\u001b[1;32m     37\u001b[0m ):\n\u001b[1;32m     38\u001b[0m     events_copy[copy_index] \u001b[38;5;241m=\u001b[39m event\n\u001b[1;32m     39\u001b[0m     copy_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loading model's parameters\n",
    "model_path = 'best_SNN_model.pth'\n",
    "model = SNN(config, False)\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Quantizing weights\n",
    "model.fc1.weight.data = quantizeFixedPoint(8, 7, model.fc1.weight.data)\n",
    "model.fc2.weight.data = quantizeFixedPoint(8, 7, model.fc2.weight.data)\n",
    "model.fc3.weight.data = quantizeFixedPoint(8, 7, model.fc3.weight.data)\n",
    "\n",
    "# Quantizing biases\n",
    "model.fc1.bias.data = quantizeFixedPoint(8, 7, model.fc1.bias.data)\n",
    "model.fc2.bias.data = quantizeFixedPoint(8, 7, model.fc2.bias.data)\n",
    "model.fc3.bias.data = quantizeFixedPoint(8, 7, model.fc3.bias.data)\n",
    "\n",
    "def test(model, test_loader, criterion, device, model_path=\"best_SNN_model.pth\"):\n",
    "\n",
    "    # Initialize variables for test loss and accuracy\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    # Switch model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over the test data\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, _ = model(data)  # Modify according to your model's output\n",
    "            outputs = outputs.mean(dim=0)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += targets.size(0)\n",
    "            correct_test += (predicted == targets).sum().item()\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cpu\")\n",
    "test_loss, test_accuracy = test(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model's parameters\n",
    "model_path = 'best_SNN_model.pth'\n",
    "model = SNN(config, False)\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Quantizing weights\n",
    "Wq = []\n",
    "Wq.append(quantizeFixedPoint(8, 7, model.fc1.weight.data))\n",
    "Wq.append(quantizeFixedPoint(8, 7, model.fc2.weight.data))\n",
    "Wq.append(quantizeFixedPoint(8, 7, model.fc3.weight.data))\n",
    "\n",
    "# Quantizing biases\n",
    "Bq = []\n",
    "Bq.append(quantizeFixedPoint(8, 7, model.fc1.bias.data))\n",
    "Bq.append(quantizeFixedPoint(8, 7, model.fc2.bias.data))\n",
    "Bq.append(quantizeFixedPoint(8, 7, model.fc3.bias.data))\n",
    "\n",
    "# Quantizing neuron parameters\n",
    "Threshq = torch.tensor([model.thresh1, model.thresh2, model.thresh3])\n",
    "Lq = Threshq * model.beta\n",
    "Threshq = quantizeFixedPoint(16, 8, Threshq)\n",
    "Lq = quantizeFixedPoint(16, 8, Lq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matMACQuantized(W, B, x):\n",
    "    \n",
    "    y = torch.matmul(W, x) + B\n",
    "    y = quantizeFixedPoint(16, 8, y)\n",
    "    return y\n",
    "\n",
    "def LifQuantized(Isyn, mem, thresh, L, beta, spike):\n",
    "    \n",
    "    # After each operation the quantizer has to be called in order to see if over/underflow has occured\n",
    "    mem = mem * beta\n",
    "    mem = quantizeFixedPoint(16, 8, mem)\n",
    "\n",
    "    # Adding L if there was a spike generated in the previous time step\n",
    "    for i in range(mem.shape[0]):\n",
    "        if spike[i] == 1:\n",
    "            mem[i] -= L\n",
    "    mem = quantizeFixedPoint(16, 8, mem)\n",
    "\n",
    "    # Rescaling Isyn\n",
    "    Isyn_trunc = torch.trunc(Isyn)\n",
    "    Isyn_trunc = quantizeFixedPoint(8, 0, Isyn_trunc)\n",
    "    Isyn = (Isyn - torch.trunc(Isyn)) + Isyn_trunc\n",
    "\n",
    "    # Add Isyn to mem\n",
    "    mem += Isyn\n",
    "    mem = quantizeFixedPoint(16, 8, mem)\n",
    "\n",
    "    spike_out = torch.zeros(mem.shape[0])\n",
    "    spike_out[mem > thresh] = 1\n",
    "\n",
    "    return spike_out, mem\n",
    "\n",
    "def runNetworkQuantized(Wq, Bq, Threshq, Lq, config, inpt, targets):\n",
    "\n",
    "    calc_target = []\n",
    "\n",
    "    for i in range(inpt.shape[1]):\n",
    "        x = inpt[:, i, :, :, :]\n",
    "    \n",
    "        mem1 = torch.zeros(config[\"dense1_input\"] // 4)\n",
    "        spike1 = torch.zeros(config[\"dense1_input\"] // 4)\n",
    "\n",
    "        mem2 = torch.zeros(config[\"dense1_input\"] // 8)\n",
    "        spike2 = torch.zeros(config[\"dense1_input\"] // 8)\n",
    "\n",
    "        mem3 = torch.zeros(config[\"num_classes\"])\n",
    "        spike3 = torch.zeros(config[\"num_classes\"])\n",
    "\n",
    "        pred = torch.zeros(config[\"num_classes\"])\n",
    "        \n",
    "        for step in range(x.shape[0]):\n",
    "        \n",
    "            current_input = x[step].flatten()\n",
    "        \n",
    "            current1 = matMACQuantized(Wq[0], Bq[0], current_input)\n",
    "            spike1, mem1 = LifQuantized(current1, mem1, Threshq[0], Lq[0], 0.5, spike1)\n",
    "\n",
    "            current2 = matMACQuantized(Wq[1], Bq[1], spike1)\n",
    "            spike2, mem2 = LifQuantized(current2, mem2, Threshq[1], Lq[1], 0.5, spike2)\n",
    "\n",
    "            current3 = matMACQuantized(Wq[2], Bq[2], spike2)\n",
    "            spike3, mem3 = LifQuantized(current3, mem3, Threshq[2], Lq[2], 0.5, spike3)\n",
    "\n",
    "            pred += spike3\n",
    "\n",
    "        # print(\"Predicted class: \", pred.argmax().item())\n",
    "        # print(\"Correct class: \", targets[i])\n",
    "        \n",
    "        calc_target.append(pred.argmax().item())\n",
    "    \n",
    "    calc_target = torch.tensor(calc_target)\n",
    "    calc_target = calc_target - targets\n",
    "    correct = calc_target.shape[0] - torch.count_nonzero(calc_target).item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Accuracy:  0.9423828125\n"
     ]
    }
   ],
   "source": [
    "input_cnt = 0\n",
    "correct_cnt = 0\n",
    "cnt = 0\n",
    "for data, targets in test_loader:\n",
    "    print(cnt)\n",
    "    input_cnt += targets.shape[0]\n",
    "    correct_cnt += runNetworkQuantized(Wq, Bq, Threshq, Lq, config, data, targets)\n",
    "    if(cnt == 15):\n",
    "        break\n",
    "    cnt += 1\n",
    "print('Accuracy: ', correct_cnt / input_cnt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNNCpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
