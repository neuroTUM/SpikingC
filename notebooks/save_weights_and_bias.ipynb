{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import snntorch as snn\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "# PyTorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "\n",
    "# Additional Imports\n",
    "import snntorch as snn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Dataset\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tonic import DiskCachedDataset\n",
    "\n",
    "# Network\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import utils\n",
    "\n",
    "import torchvision.transforms as torchvision_transforms\n",
    "import tonic\n",
    "import tonic.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([31, 128, 2, 34, 34])\n",
      "Targets shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Define sensor size for NMNIST dataset\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "\n",
    "# Define transformations\n",
    "# Note: The use of torch.from_numpy is removed as Tonic's transforms handle conversion.\n",
    "transform = tonic.transforms.Compose([\n",
    "    transforms.Denoise(filter_time=10000),\n",
    "    transforms.ToFrame(sensor_size=sensor_size, time_window=10000),\n",
    "    # torchvision.transforms.RandomRotation is not directly applicable to event data.\n",
    "    # If rotation is needed, it should be done on the frames after conversion by ToFrame.\n",
    "])\n",
    "\n",
    "# Load NMNIST datasets without caching\n",
    "trainset = tonic.datasets.NMNIST(save_to='./tmp/data', transform=transform, train=True)\n",
    "testset = tonic.datasets.NMNIST(save_to='./tmp/data', transform=transform, train=False)\n",
    "\n",
    "# Split trainset into training and validation datasets\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training, validation, and testing\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "\n",
    "# Fetch a single batch from the train_loader to inspect the shape\n",
    "data, targets = next(iter(train_loader))\n",
    "print(f\"Data shape: {data.shape}\")  # Example output: torch.Size([batch_size, timesteps, channels, height, width])\n",
    "print(f\"Targets shape: {targets.shape}\")  # Example output: torch.Size([batch_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # SNN\n",
    "    \"threshold1\": 2.5,\n",
    "    \"threshold2\": 8.0,\n",
    "    \"threshold3\": 4.0,\n",
    "    \"threshold4\": 2.0,\n",
    "    \"beta\": 0.5,\n",
    "    \"num_steps\": 10,\n",
    "    \n",
    "    # SNN Dense Shape\n",
    "    \"dense1_input\": 2312,\n",
    "    \"num_classes\": 10,\n",
    "    \n",
    "\n",
    "    # Network\n",
    "    \"batch_norm\": True,\n",
    "    \"dropout\": 0.3,\n",
    "\n",
    "    # Hyper Params\n",
    "    \"lr\": 0.007,\n",
    "\n",
    "    # Early Stopping\n",
    "    \"min_delta\": 1e-6,\n",
    "    \"patience_es\": 20,\n",
    "\n",
    "    # Training\n",
    "    \"epochs\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/copparihollmann/miniconda3/envs/SNNCpp/lib/python3.12/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "class SNN_tests(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super(SNN_tests, self).__init__()\n",
    "\n",
    "    # Initialize configuration parameters\n",
    "      # LIF\n",
    "    self.thresh1 = config[\"threshold1\"]\n",
    "    self.thresh2 = config[\"threshold2\"]\n",
    "    self.thresh3 = config[\"threshold3\"]\n",
    "    self.thresh4 = config[\"threshold4\"]\n",
    "    self.beta = config[\"beta\"]\n",
    "    self.num_steps = config[\"num_steps\"]\n",
    "\n",
    "      # Hyper Params for Layers\n",
    "    self.batch_norm = config[\"batch_norm\"]\n",
    "    self.dropout_percent = config[\"dropout\"]\n",
    "\n",
    "      # Dense Shape\n",
    "    self.dense1_input = config[\"dense1_input\"]\n",
    "    self.num_classes = config[\"num_classes\"]\n",
    "\n",
    "      # Network Layers\n",
    "    self.fc1 = nn.Linear(self.dense1_input, self.dense1_input//4)\n",
    "    self.lif1 = snn.Leaky(beta=self.beta, threshold=self.thresh1)\n",
    "    \n",
    "    \n",
    "    self.fc2 = nn.Linear(self.dense1_input//4, self.dense1_input//8)\n",
    "    self.lif2 = snn.Leaky(beta=self.beta, threshold=self.thresh2)\n",
    "    \n",
    "    self.fc3 = nn.Linear(self.dense1_input//8, self.num_classes)\n",
    "    self.lif3 = snn.Leaky(beta=self.beta, threshold=self.thresh3)\n",
    "    \n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc4 = nn.Linear(self.dense1_input//8, self.num_classes)\n",
    "    self.lif4 = snn.Leaky(beta=self.beta, threshold=self.thresh4)\n",
    "    self.dropout = nn.Dropout(self.dropout_percent)\n",
    "    \n",
    "    # Extra (not used)\n",
    "    self.batch_norm_1 = nn.BatchNorm2d(num_features=16)\n",
    "    self.batch_norm_2 = nn.BatchNorm2d(num_features=32)\n",
    "    \n",
    "    \n",
    "    # Forward Pass\n",
    "  def forward(self, inpt):\n",
    "    mem1 = self.lif1.init_leaky()\n",
    "    mem2 = self.lif2.init_leaky()\n",
    "    mem3 = self.lif3.init_leaky()\n",
    "\n",
    "    all_outputs = {layer: [] for layer in ['inputs', 'fc1_outputs', 'lif1_spikes', 'fc2_outputs', 'lif2_spikes', 'fc3_outputs', 'lif3_spikes', 'mem1', 'mem2', 'mem3']}\n",
    "    print(inpt.shape)\n",
    "    for step in range(inpt.shape[0]):  # assuming inpt shape is (batch, time, ...)\n",
    "        current_input = inpt[step, ...].to(device)\n",
    "        \n",
    "        current_input = self.flatten(current_input)\n",
    "\n",
    "        current1 = self.fc1(current_input)\n",
    "        spike1, mem1 = self.lif1(current1, mem1)\n",
    "        current2 = self.fc2(spike1)\n",
    "        spike2, mem2 = self.lif2(current2, mem2)\n",
    "        current3 = self.fc3(spike2)\n",
    "        spike3, mem3 = self.lif3(current3, mem3)\n",
    "\n",
    "        # Collect outputs for each step\n",
    "        all_outputs['inputs'].append(current_input.detach().cpu().numpy())\n",
    "        all_outputs['fc1_outputs'].append(current1.detach().cpu().numpy())\n",
    "        all_outputs['lif1_spikes'].append(spike1.detach().cpu().numpy())\n",
    "        all_outputs['fc2_outputs'].append(current2.detach().cpu().numpy())\n",
    "        all_outputs['lif2_spikes'].append(spike2.detach().cpu().numpy())\n",
    "        all_outputs['fc3_outputs'].append(current3.detach().cpu().numpy())\n",
    "        all_outputs['lif3_spikes'].append(spike3.detach().cpu().numpy())\n",
    "        all_outputs['mem1'].append(mem1.detach().cpu().numpy())\n",
    "        all_outputs['mem2'].append(mem2.detach().cpu().numpy())\n",
    "        all_outputs['mem3'].append(mem3.detach().cpu().numpy())\n",
    "\n",
    "    return all_outputs\n",
    "  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SNN_tests(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31, 1, 2, 34, 34])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_and_save_model_outputs(model_path, test_loader, device):\n",
    "    model = SNN_tests(config).to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Load the model weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    # Grab a single sample from the loader\n",
    "    single_sample, _ = next(iter(test_loader))  # Adjust if your dataloader is different\n",
    "    single_sample = single_sample.to(device)\n",
    "\n",
    "    # Run the forward pass\n",
    "    layer_outputs = model(single_sample[:,0, ...].unsqueeze(1))  # Adjust slicing based on your data\n",
    "\n",
    "    # Save the outputs\n",
    "    save_layer_outputs(layer_outputs)\n",
    "\n",
    "def save_layer_outputs(layer_outputs):\n",
    "    base_dir = \"./intermediate_outputs\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "    \n",
    "    for layer_name, timesteps_outputs in layer_outputs.items():\n",
    "        layer_dir = os.path.join(base_dir, layer_name)\n",
    "        if not os.path.exists(layer_dir):\n",
    "            os.makedirs(layer_dir)\n",
    "        \n",
    "        for step, output in enumerate(timesteps_outputs):\n",
    "            file_path = os.path.join(layer_dir, f\"{layer_name}_timestep_{step}.bin\")\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(output, f)\n",
    "\n",
    "# Assuming the model and device are defined, and model's forward() is correctly returning layer outputs\n",
    "# Example Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"/home/copparihollmann/neuroTUM/SpikingC/SpikingCpp/notebooks/best_SNN_model.pth\"\n",
    "load_and_save_model_outputs(model_path, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super(SNN, self).__init__()\n",
    "\n",
    "    # Initialize configuration parameters\n",
    "      # LIF\n",
    "    self.thresh1 = config[\"threshold1\"]\n",
    "    self.thresh2 = config[\"threshold2\"]\n",
    "    self.thresh3 = config[\"threshold3\"]\n",
    "    self.beta = config[\"beta\"]\n",
    "    self.num_steps = config[\"num_steps\"]\n",
    "\n",
    "      # Hyper Params for Layers\n",
    "    self.batch_norm = config[\"batch_norm\"]\n",
    "    self.dropout_percent = config[\"dropout\"]\n",
    "\n",
    "      # Dense Shape\n",
    "    self.dense1_input = config[\"dense1_input\"]\n",
    "    self.num_classes = config[\"num_classes\"]\n",
    "\n",
    "      # Network Layers\n",
    "    self.fc1 = nn.Linear(self.dense1_input, self.dense1_input//4)\n",
    "    self.lif1 = snn.Leaky(beta=self.beta, threshold=self.thresh1)\n",
    "    \n",
    "    \n",
    "    self.fc2 = nn.Linear(self.dense1_input//4, self.dense1_input//8)\n",
    "    self.lif2 = snn.Leaky(beta=self.beta, threshold=self.thresh2)\n",
    "    \n",
    "    self.fc3 = nn.Linear(self.dense1_input//8, self.num_classes)\n",
    "    self.lif3 = snn.Leaky(beta=self.beta, threshold=self.thresh3)\n",
    "    \n",
    "    self.flatten = nn.Flatten()\n",
    "    \n",
    "    \n",
    "    # Forward Pass\n",
    "  def forward(self, inpt):\n",
    "    mem1 = self.lif1.init_leaky()\n",
    "    mem2 = self.lif2.init_leaky()\n",
    "    mem3 = self.lif3.init_leaky()\n",
    "    #mem4 = self.lif4.init_leaky()\n",
    "\n",
    "    spike3_rec = []\n",
    "    mem3_rec = []\n",
    "    \n",
    "    #print(inpt.shape)\n",
    "\n",
    "    for step in range(inpt.shape[0]):\n",
    "      #print(inpt[step].shape)\n",
    "      \n",
    "      current_input = inpt[step]\n",
    "      current_input = self.flatten(current_input)\n",
    "      \n",
    "      current1 = self.fc1(current_input)\n",
    "      spike1, mem1 = self.lif1(current1, mem1)\n",
    "\n",
    "      current2 = self.fc2(spike1)\n",
    "      spike2, mem2 = self.lif2(current2, mem2)\n",
    "\n",
    "      current3 = self.fc3(spike2)\n",
    "      spike3, mem3 = self.lif3(current3, mem3)\n",
    "      \n",
    "      #current4 = self.fc4(spike3)\n",
    "      #spike4, mem4 = self.lif4(current4, mem4)\n",
    "\n",
    "      spike3_rec.append(spike3)\n",
    "      mem3_rec.append(mem3)\n",
    "\n",
    "    return torch.stack(spike3_rec, dim=0), torch.stack(mem3_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights_and_biases_to_binary(model_path, save_directory=\"model_params_binary\"):\n",
    "    # Load the model's state dictionary\n",
    "    state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    # Save each parameter to a separate binary file\n",
    "    for name, param in state_dict.items():\n",
    "        file_path = os.path.join(save_directory, f\"{name.replace('.', '_')}.bin\")\n",
    "        # Open the file in binary write mode and save the tensor data\n",
    "        with open(file_path, 'wb') as f:\n",
    "            # Convert the tensor to numpy and write as a binary stream\n",
    "            numpy_array = param.numpy()\n",
    "            f.write(numpy_array.tobytes())\n",
    "\n",
    "# Example usage\n",
    "model_path = \"/home/copparihollmann/neuroTUM/SpikingC/SpikingCpp/notebooks/best_SNN_model.pth\"\n",
    "save_weights_and_biases_to_binary(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save weights and biases as binary files\n",
    "def save_weights_and_biases_binary(model, directory=\"model_params\"):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    for name, param in model.named_parameters():\n",
    "        # Construct file path\n",
    "        file_path = os.path.join(directory, f\"{name.replace('.', '_')}.bin\")\n",
    "        # Open file in binary write mode and save using pickle\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(param.detach().cpu().numpy(), f)\n",
    "\n",
    "# Function to save layer outputs as binary files\n",
    "def save_layer_outputs_binary(model, single_sample, directory=\"layer_outputs\"):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    # Ensure single_sample is on the correct device and has batch dimension\n",
    "    single_sample = single_sample.to(device).unsqueeze(0)\n",
    "    \n",
    "    # Run the model and get outputs\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        layer_outputs = model(single_sample)\n",
    "    \n",
    "    # Save each layer's outputs as a binary file\n",
    "    for layer_name, output in layer_outputs.items():\n",
    "        # Flatten output and convert to numpy if it's a tensor\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            output = output.detach().cpu().numpy().flatten()\n",
    "        \n",
    "        # Construct file path\n",
    "        file_path = os.path.join(directory, f\"{layer_name.replace('.', '_')}.bin\")\n",
    "        # Open file in binary write mode and save using pickle\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(output, f)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Assume 'model' is a PyTorch model and 'single_sample' is a tensor representing a single data sample\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "single_sample = torch.rand((1, 3, 224, 224))  # Example tensor shape for an image\n",
    "single_sample = single_sample.to(device)\n",
    "\n",
    "# Save weights and biases as binary files\n",
    "save_weights_and_biases_binary(model)\n",
    "\n",
    "# Save layer outputs as binary files\n",
    "save_layer_outputs_binary(model, single_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 2, 34, 34])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'tobytes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m SNN_tests(config)\n\u001b[1;32m     45\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m34\u001b[39m, \u001b[38;5;241m34\u001b[39m))  \u001b[38;5;66;03m# Example input tensor with 30 timesteps\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43msave_layer_outputs_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 39\u001b[0m, in \u001b[0;36msave_layer_outputs_binary\u001b[0;34m(model, input_tensor, device, base_dir)\u001b[0m\n\u001b[1;32m     37\u001b[0m timestep_output_flat \u001b[38;5;241m=\u001b[39m timestep_output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 39\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mtimestep_output_flat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m())\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m output at timestep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'tobytes'"
     ]
    }
   ],
   "source": [
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def save_layer_outputs_binary(model, input_tensor, device, base_dir=\"intermediate_outputs\"):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    # Prepare the model to save outputs\n",
    "    saved_outputs = {}\n",
    "\n",
    "    def hook_fn(module, input, output, name):\n",
    "        if name not in saved_outputs:\n",
    "            saved_outputs[name] = []\n",
    "        saved_outputs[name].append(output)\n",
    "\n",
    "    hooks = []\n",
    "    for name, module in model.named_modules():\n",
    "        if not isinstance(module, nn.Sequential) and not isinstance(module, SNN_tests) and module != model:\n",
    "            hooks.append(module.register_forward_hook(lambda module, input, output, name=name: hook_fn(module, input, output, name)))\n",
    "\n",
    "    # Run model forward pass\n",
    "    _ = model(input_tensor)\n",
    "\n",
    "    # Remove hooks after use\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # Save outputs to binary files for each timestep\n",
    "    for name, outputs in saved_outputs.items():\n",
    "        layer_dir = os.path.join(base_dir, name)\n",
    "        ensure_dir(layer_dir)\n",
    "        for timestep, timestep_output in enumerate(outputs):\n",
    "            file_name = os.path.join(layer_dir, f\"output_timestep_{timestep}.bin\")\n",
    "            timestep_output_flat = timestep_output.flatten()\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(timestep_output_flat.tobytes())\n",
    "            print(f\"Saved {name} output at timestep {timestep} to {file_name}\")\n",
    "\n",
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SNN_tests(config)\n",
    "input_tensor = torch.randn((30, 2, 34, 34))  # Example input tensor with 30 timesteps\n",
    "\n",
    "save_layer_outputs_binary(model, input_tensor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def load_binary_file_to_tensor(file_path, dtype=np.float32, shape=(31, 2, 34, 34)):\n",
    "    # Read the binary file\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = np.fromfile(f, dtype=dtype)\n",
    "\n",
    "    # Reshape the data to match the input shape\n",
    "    data = data.reshape(shape)\n",
    "\n",
    "    # Convert the NumPy array to a PyTorch tensor\n",
    "    tensor = torch.tensor(data, dtype=torch.float32)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "# Use the function to load the binary file into a tensor\n",
    "file_path = \"/home/copparihollmann/neuroTUM/SpikingCpp/notebooks/tmp/data/NMNIST/Test/7/00001.bin\"\n",
    "input_tensor = load_binary_file_to_tensor(file_path)\n",
    "\n",
    "# Now you can process this tensor in a loop for each time step\n",
    "for timestep in range(input_tensor.shape[0]):\n",
    "    timestep_data = input_tensor[timestep]  # This selects the data for the current timestep\n",
    "    # Process the data for the current timestep\n",
    "    # For example, pass it through your model\n",
    "    output = model(timestep_data.unsqueeze(0))  # Add batch dimension if needed\n",
    "\n",
    "    # ... Save or process the output as needed ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assume 'weights' is your pre-trained weights tensor\n",
    "weights = model.fc1.weight.data  # example to get the weights from the first fully connected layer\n",
    "\n",
    "# Compute the scale and zero-point for quantization\n",
    "scale = (weights.max() - weights.min()) / 255\n",
    "zero_point = weights.min()\n",
    "\n",
    "# Quantize the weights to integers\n",
    "quantized_weights = torch.round((weights - zero_point) / scale).to(torch.uint8)\n",
    "\n",
    "# Dequantize back to float to use in computations (simulate quantized operations)\n",
    "dequantized_weights = quantized_weights.float() * scale + zero_point\n",
    "\n",
    "# Now replace the original weights with the dequantized weights\n",
    "model.fc1.weight.data = dequantized_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def quantize_weights_to_int8(weights):\n",
    "    \"\"\"\n",
    "    Quantizes the given weights to 8-bit integer values.\n",
    "\n",
    "    :param weights: The floating-point weights of a neural network layer.\n",
    "    :return: A tuple of (quantized_weights, scale, zero_point), where\n",
    "        quantized_weights is an ndarray of int8 values representing the quantized weights,\n",
    "        scale is the scaling factor used for quantization,\n",
    "        zero_point is the shift used to align the floating point zero to the center of the int8 range.\n",
    "    \"\"\"\n",
    "    # Calculate the range of the weight values\n",
    "    min_val, max_val = weights.min(), weights.max()\n",
    "    \n",
    "    # Calculate scale and zero_point for the quantization\n",
    "    scale = (max_val - min_val) / 255\n",
    "    zero_point = round(128 - max_val / scale)\n",
    "\n",
    "    # Quantize weights to integer values\n",
    "    quantized_weights = np.round(weights / scale + zero_point).astype(np.int8)\n",
    "\n",
    "    # Ensure zero_point is set such that 0.0 maps to an integer value of zero\n",
    "    quantized_weights = np.clip(quantized_weights, -128, 127)\n",
    "\n",
    "    return quantized_weights, scale, zero_point\n",
    "\n",
    "# Dummy weights for demonstration\n",
    "original_weights = np.random.randn(3, 3).astype(np.float32)\n",
    "\n",
    "# Perform quantization\n",
    "quantized_weights, scale, zero_point = quantize_weights_to_int8(original_weights)\n",
    "\n",
    "# Display results\n",
    "print(\"Original weights:\")\n",
    "print(original_weights)\n",
    "print(\"\\nQuantized weights:\")\n",
    "print(quantized_weights)\n",
    "print(\"\\nScale:\", scale)\n",
    "print(\"Zero point:\", zero_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.quantization\n",
    "\n",
    "# Assume 'model' is your pre-trained model\n",
    "model.eval()\n",
    "\n",
    "# Specify the quantization configuration\n",
    "# In this case, we are using static quantization\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Insert observers to collect statistics on the model's weights\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "# Calibrate the model with representative data\n",
    "with torch.no_grad():\n",
    "    for input, _ in representative_dataset:\n",
    "        model(input)  # Use the model's forward method to perform calibration\n",
    "\n",
    "# Convert the model to a quantized version\n",
    "torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "# Now 'model' is quantized, and you can save, load, or run inference with it\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuromorphic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
