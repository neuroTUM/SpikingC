{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple SNN Network for NMNIST Dataset Classification\n",
    "\n",
    "The goal of this script is to train and test a simple SNN network to then extract the weights and map them to a simple SNN network in C++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1b8bb5a250>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "\n",
    "# Additional Imports\n",
    "import snntorch as snn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Dataset\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tonic import DiskCachedDataset\n",
    "\n",
    "# Network\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import utils\n",
    "\n",
    "# Set the seed for reproducibility of results\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. NMNIST Dataset Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tonic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtonic\u001b[49m\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mNMNIST(save_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m events, target \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(events)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tonic' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = tonic.datasets.NMNIST(save_to='./data', train=True)\n",
    "events, target = dataset[0]\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16336/305858719.py:1: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  tonic.utils.plot_event_grid(events)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tonic.utils.plot_event_grid(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([31, 128, 2, 34, 34])\n",
      "Targets shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as torchvision_transforms\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "\n",
    "# Define sensor size for NMNIST dataset\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "\n",
    "# Define transformations\n",
    "# Note: The use of torch.from_numpy is removed as Tonic's transforms handle conversion.\n",
    "transform = tonic.transforms.Compose([\n",
    "    transforms.Denoise(filter_time=10000),\n",
    "    transforms.ToFrame(sensor_size=sensor_size, time_window=10000),\n",
    "    # torchvision.transforms.RandomRotation is not directly applicable to event data.\n",
    "    # If rotation is needed, it should be done on the frames after conversion by ToFrame.\n",
    "])\n",
    "\n",
    "# Load NMNIST datasets without caching\n",
    "trainset = tonic.datasets.NMNIST(save_to='./tmp/data', transform=transform, train=True)\n",
    "testset = tonic.datasets.NMNIST(save_to='./tmp/data', transform=transform, train=False)\n",
    "\n",
    "# Split trainset into training and validation datasets\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training, validation, and testing\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "\n",
    "# Fetch a single batch from the train_loader to inspect the shape\n",
    "data, targets = next(iter(train_loader))\n",
    "print(f\"Data shape: {data.shape}\")  # Example output: torch.Size([batch_size, timesteps, channels, height, width])\n",
    "print(f\"Targets shape: {targets.shape}\")  # Example output: torch.Size([batch_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([31, 128, 2, 34, 34])\n",
      "Targets shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Fetch a single batch from the train_loader\n",
    "data, targets = next(iter(train_loader))\n",
    "\n",
    "# Assuming your data is in the format [timesteps, batch_size, channels, height, width]\n",
    "# Check the shape of the data tensor\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "# Check the shape of the targets tensor\n",
    "print(f\"Targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # SNN\n",
    "    \"threshold1\": 2.5,\n",
    "    \"threshold2\": 8.0,\n",
    "    \"threshold3\": 4.0,\n",
    "    \"threshold4\": 2.0,\n",
    "    \"beta\": 0.5,\n",
    "    \"num_steps\": 10,\n",
    "    \n",
    "    # SNN Dense Shape\n",
    "    \"dense1_input\": 2312,\n",
    "    \"num_classes\": 10,\n",
    "    \n",
    "\n",
    "    # Network\n",
    "    \"batch_norm\": True,\n",
    "    \"dropout\": 0.3,\n",
    "\n",
    "    # Hyper Params\n",
    "    \"lr\": 0.007,\n",
    "\n",
    "    # Early Stopping\n",
    "    \"min_delta\": 1e-6,\n",
    "    \"patience_es\": 20,\n",
    "\n",
    "    # Training\n",
    "    \"epochs\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSNN\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(SNN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class SNN(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super(SNN, self).__init__()\n",
    "\n",
    "    # Initialize configuration parameters\n",
    "      # LIF\n",
    "    self.thresh1 = config[\"threshold1\"]\n",
    "    self.thresh2 = config[\"threshold2\"]\n",
    "    self.thresh3 = config[\"threshold3\"]\n",
    "    self.thresh4 = config[\"threshold4\"]\n",
    "    self.beta = config[\"beta\"]\n",
    "    self.num_steps = config[\"num_steps\"]\n",
    "\n",
    "      # Hyper Params for Layers\n",
    "    self.batch_norm = config[\"batch_norm\"]\n",
    "    self.dropout_percent = config[\"dropout\"]\n",
    "\n",
    "      # Dense Shape\n",
    "    self.dense1_input = config[\"dense1_input\"]\n",
    "    self.num_classes = config[\"num_classes\"]\n",
    "\n",
    "      # Network Layers\n",
    "    self.fc1 = nn.Linear(self.dense1_input, self.dense1_input//4)\n",
    "    self.lif1 = snn.Leaky(beta=self.beta, threshold=self.thresh1)\n",
    "    \n",
    "    \n",
    "    self.fc2 = nn.Linear(self.dense1_input//4, self.dense1_input//8)\n",
    "    self.lif2 = snn.Leaky(beta=self.beta, threshold=self.thresh2)\n",
    "    \n",
    "    self.fc3 = nn.Linear(self.dense1_input//8, self.num_classes)\n",
    "    self.lif3 = snn.Leaky(beta=self.beta, threshold=self.thresh3)\n",
    "    \n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc4 = nn.Linear(self.dense1_input//8, self.num_classes)\n",
    "    self.lif4 = snn.Leaky(beta=self.beta, threshold=self.thresh4)\n",
    "    self.dropout = nn.Dropout(self.dropout_percent)\n",
    "    \n",
    "    # Extra (not used)\n",
    "    self.batch_norm_1 = nn.BatchNorm2d(num_features=16)\n",
    "    self.batch_norm_2 = nn.BatchNorm2d(num_features=32)\n",
    "    \n",
    "    \n",
    "    # Forward Pass\n",
    "  def forward(self, inpt):\n",
    "    mem1 = self.lif1.init_leaky()\n",
    "    mem2 = self.lif2.init_leaky()\n",
    "    mem3 = self.lif3.init_leaky()\n",
    "    #mem4 = self.lif4.init_leaky()\n",
    "\n",
    "    spike3_rec = []\n",
    "    mem3_rec = []\n",
    "    \n",
    "    #print(inpt.shape)\n",
    "\n",
    "    for step in range(inpt.shape[0]):\n",
    "      #print(inpt[step].shape)\n",
    "      \n",
    "      current_input = inpt[step]\n",
    "      current_input = self.flatten(current_input)\n",
    "      \n",
    "      current1 = self.fc1(current_input)\n",
    "      spike1, mem1 = self.lif1(current1, mem1)\n",
    "\n",
    "      current2 = self.fc2(spike1)\n",
    "      spike2, mem2 = self.lif2(current2, mem2)\n",
    "\n",
    "      current3 = self.fc3(spike2)\n",
    "      spike3, mem3 = self.lif3(current3, mem3)\n",
    "      \n",
    "      #current4 = self.fc4(spike3)\n",
    "      #spike4, mem4 = self.lif4(current4, mem4)\n",
    "\n",
    "      spike3_rec.append(spike3)\n",
    "      mem3_rec.append(mem3)\n",
    "\n",
    "    return torch.stack(spike3_rec, dim=0), torch.stack(mem3_rec, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=config[\"patience_es\"], min_delta=config[\"min_delta\"]):\n",
    "        # Early stops the training if validation loss doesn't improve after a given patience.\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "        elif val_loss > self.best_score - self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"Earlystop {self.counter}/{self.patience}\\n\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Training Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Model initialization\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m SNN(config)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Optimizer and Loss Function\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Model initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SNN(config).to(device)\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "optimizer = Adam(model.parameters(), lr=config[\"lr\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(patience=config[\"patience_es\"], min_delta=config[\"min_delta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for data, targets in train_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        #print(data.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        spike_out, _ = model(data)\n",
    "        output = spike_out.sum(dim=0)\n",
    "        loss = criterion(output, targets)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted_train = torch.max(output.data, 1)\n",
    "        total_train += targets.size(0)\n",
    "        correct_train += (predicted_train == targets).sum().item()\n",
    "        \n",
    "        print(f\"Train Loss: {loss.item():.2f}\")\n",
    "        \n",
    "        acc = SF.accuracy_rate(spike_out, targets) \n",
    "        print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in val_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            spike_out, _ = model(data)\n",
    "            output = spike_out.sum(dim=0)\n",
    "            loss = criterion(output, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted_val = torch.max(output.data, 1)\n",
    "            total_val += targets.size(0)\n",
    "            correct_val += (predicted_val == targets).sum().item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      3\u001b[0m train_losses, train_accuracies, val_losses, val_accuracies \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[1;32m      4\u001b[0m best_val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = [], [], [], []\n",
    "best_val_accuracy = 0\n",
    "model_path = \"best_SNN_model.pth\"\n",
    "\n",
    "for epoch in tqdm(range(config[\"epochs\"]), desc=\"Epochs\"):\n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    tqdm.write(f\"Epoch: {epoch + 1}, Training Loss: {train_loss:.5f}, Training Accuracy: {train_accuracy:.2f}%, \"\n",
    "               f\"Validation Loss: {val_loss:.5f}, Validation Accuracy: {val_accuracy:.2f}%\\n\")\n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        tqdm.write(f\"Saved model with improved validation accuracy: {val_accuracy:.2f}% \\n\")\n",
    "\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        tqdm.write(\"\\nEarly stopping triggered\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plotting training, validation, and test losses\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(val_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plotting training, validation, and test losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting training, validation, and test accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, device, model_path=\"best_SNN_model.pth\"):\n",
    "\n",
    "    # Initialize variables for test loss and accuracy\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    # Restore best SNN Model\n",
    "    if os.path.isfile(model_path):\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(f\"Loaded saved model from {model_path}\\n\")\n",
    "\n",
    "    # Switch model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over the test data\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, _ = model(data)  # Modify according to your model's output\n",
    "            outputs = outputs.mean(dim=0)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += targets.size(0)\n",
    "            correct_test += (predicted == targets).sum().item()\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "\n",
    "    return test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m test(\u001b[43mmodel\u001b[49m, test_loader, criterion, device)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = test(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Save weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m SNN(config)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the trained model weights\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SNN(config).to(device)\n",
    "\n",
    "# Load the trained model weights\n",
    "model_path = \"best_SNN_model.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Ensure the model is in evaluation mode if it uses layers like dropout or batch normalization\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the trained model weights\n",
    "model_path = \"best_SNN_model.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param_data = param.detach().cpu().numpy()  # Keep the original shape\n",
    "    if \"weight\" in name:\n",
    "        # Save weights with details\n",
    "        for output_neuron_index, weights_to_neuron in enumerate(param_data):\n",
    "            df = pd.DataFrame(weights_to_neuron.reshape(1, -1))  # One output neuron's weights per row\n",
    "            formatted_name = f\"{name}_to_output_neuron_{output_neuron_index}.csv\"\n",
    "            formatted_name = formatted_name.replace(\".\", \"_\")  # Format the filename\n",
    "            df.to_csv(formatted_name, index=False, header=False)\n",
    "    elif \"bias\" in name:\n",
    "        # Save biases directly, as they are already 1D\n",
    "        df = pd.DataFrame(param_data.reshape(1, -1))  # All biases in one row\n",
    "        formatted_name = name.replace(\".\", \"_\") + \".csv\"\n",
    "        df.to_csv(formatted_name, index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the trained model weights\n",
    "model_path = \"best_SNN_model.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param_data = param.detach().cpu().numpy()  # Keep the original shape\n",
    "    \n",
    "    if \"weight\" in name:\n",
    "        # Save all weights for the layer in one CSV\n",
    "        df_weights = pd.DataFrame(param_data)  # Rows: output neurons, Columns: input neurons\n",
    "        formatted_name_weights = name.replace(\".\", \"_\") + \"_weights.csv\"\n",
    "        df_weights.to_csv(formatted_name_weights, index=False, header=False)\n",
    "        \n",
    "    elif \"bias\" in name:\n",
    "        # Save biases for the layer in one CSV\n",
    "        df_bias = pd.DataFrame(param_data).T  # Convert to row vector for consistency\n",
    "        formatted_name_bias = name.replace(\".\", \"_\") + \"_bias.csv\"\n",
    "        df_bias.to_csv(formatted_name_bias, index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "# Load the trained model weights\n",
    "model_path = \"best_SNN_model.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "model_parameters = {}\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # Convert parameter to numpy array and then to list for JSON serialization\n",
    "    param_data = param.detach().cpu().numpy().tolist()  # Use tolist() to make JSON serializable\n",
    "    \n",
    "    # Assign parameter data to the corresponding layer name in the dictionary\n",
    "    model_parameters[name] = param_data\n",
    "\n",
    "# Define file path for JSON file\n",
    "json_file_path = \"model_parameters.json\"\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(model_parameters, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights to CSV files\n",
    "for name, param in model.named_parameters():\n",
    "    # Convert the tensor to a numpy array and flatten it\n",
    "    param_data = param.detach().cpu().numpy().flatten()\n",
    "    # Create a DataFrame and save it as CSV\n",
    "    df = pd.DataFrame(param_data)\n",
    "    # Format the filename to avoid issues with directory separators\n",
    "    formatted_name = name.replace(\".\", \"_\")\n",
    "    df.to_csv(f\"{formatted_name}.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Placeholder for all parameter data\n",
    "all_params = []\n",
    "\n",
    "# Collect parameter data\n",
    "for name, param in model.named_parameters():\n",
    "    param_data = param.detach().cpu().numpy().flatten()\n",
    "    for value in param_data:\n",
    "        all_params.append([name, value])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_params = pd.DataFrame(all_params, columns=['Parameter', 'Value'])\n",
    "\n",
    "# Save to CSV\n",
    "df_params.to_csv(\"model_weights.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "model_weights = {}\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # Convert the tensor to a list\n",
    "    param_data = param.detach().cpu().numpy().flatten().tolist()\n",
    "    model_weights[name] = param_data\n",
    "\n",
    "# Save to JSON\n",
    "with open('model_weights.json', 'w') as f:\n",
    "    json.dump(model_weights, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Save intermediate results from each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN_tests(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super(SNN_tests, self).__init__()\n",
    "\n",
    "    # Initialize configuration parameters\n",
    "      # LIF\n",
    "    self.thresh1 = config[\"threshold1\"]\n",
    "    self.thresh2 = config[\"threshold2\"]\n",
    "    self.thresh3 = config[\"threshold3\"]\n",
    "    self.thresh4 = config[\"threshold4\"]\n",
    "    self.beta = config[\"beta\"]\n",
    "    self.num_steps = config[\"num_steps\"]\n",
    "\n",
    "      # Hyper Params for Layers\n",
    "    self.batch_norm = config[\"batch_norm\"]\n",
    "    self.dropout_percent = config[\"dropout\"]\n",
    "\n",
    "      # Dense Shape\n",
    "    self.dense1_input = config[\"dense1_input\"]\n",
    "    self.num_classes = config[\"num_classes\"]\n",
    "\n",
    "      # Network Layers\n",
    "    self.fc1 = nn.Linear(self.dense1_input, self.dense1_input//4)\n",
    "    self.lif1 = snn.Leaky(beta=self.beta, threshold=self.thresh1)\n",
    "    \n",
    "    \n",
    "    self.fc2 = nn.Linear(self.dense1_input//4, self.dense1_input//8)\n",
    "    self.lif2 = snn.Leaky(beta=self.beta, threshold=self.thresh2)\n",
    "    \n",
    "    self.fc3 = nn.Linear(self.dense1_input//8, self.num_classes)\n",
    "    self.lif3 = snn.Leaky(beta=self.beta, threshold=self.thresh3)\n",
    "    \n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc4 = nn.Linear(self.dense1_input//8, self.num_classes)\n",
    "    self.lif4 = snn.Leaky(beta=self.beta, threshold=self.thresh4)\n",
    "    self.dropout = nn.Dropout(self.dropout_percent)\n",
    "    \n",
    "    # Extra (not used)\n",
    "    self.batch_norm_1 = nn.BatchNorm2d(num_features=16)\n",
    "    self.batch_norm_2 = nn.BatchNorm2d(num_features=32)\n",
    "    \n",
    "    \n",
    "    # Forward Pass\n",
    "  def forward(self, inpt):\n",
    "    mem1 = self.lif1.init_leaky()\n",
    "    mem2 = self.lif2.init_leaky()\n",
    "    mem3 = self.lif3.init_leaky()\n",
    "    #mem4 = self.lif4.init_leaky()\n",
    "\n",
    "    spike3_rec = []\n",
    "    mem3_rec = []\n",
    "    \n",
    "    all_outputs = {\n",
    "            'inputs': [],\n",
    "            'fc1_outputs': [],\n",
    "            'lif1_spikes': [],\n",
    "            'fc2_outputs': [],\n",
    "            'lif2_spikes': [],\n",
    "            'fc3_outputs': [],\n",
    "            'lif3_spikes': [],\n",
    "            'mem1': [],\n",
    "            'mem2': [],\n",
    "            'mem3': []\n",
    "        }\n",
    "    \n",
    "    print(inpt.shape)\n",
    "\n",
    "    for step in range(inpt.shape[0]):\n",
    "        current_input = inpt[step]\n",
    "        #current_input = self.flatten(current_input)\n",
    "        current_input = torch.Tensor(current_input).to(device)\n",
    "        current_input = current_input.unsqueeze(0)\n",
    "        current_input = self.flatten(current_input)\n",
    "        \n",
    "        all_outputs['inputs'].append(current_input.cpu().numpy())  # Saving input\n",
    "\n",
    "        current1 = self.fc1(current_input)\n",
    "        spike1, mem1 = self.lif1(current1, mem1)\n",
    "        all_outputs['fc1_outputs'].append(current1.detach().cpu().numpy())  # FC1 output\n",
    "        all_outputs['lif1_spikes'].append(spike1.detach().cpu().numpy())  # LIF1 spikes\n",
    "        all_outputs['mem1'].append(mem1.detach().cpu().numpy())\n",
    "\n",
    "        current2 = self.fc2(spike1)\n",
    "        spike2, mem2 = self.lif2(current2, mem2)\n",
    "        all_outputs['fc2_outputs'].append(current2.detach().cpu().numpy())  # FC2 output\n",
    "        all_outputs['lif2_spikes'].append(spike2.detach().cpu().numpy())  # LIF2 spikes\n",
    "        all_outputs['mem2'].append(mem2.detach().cpu().numpy())\n",
    "\n",
    "        current3 = self.fc3(spike2)\n",
    "        spike3, mem3 = self.lif3(current3, mem3)\n",
    "        all_outputs['fc3_outputs'].append(current3.detach().cpu().numpy())  # FC3 output\n",
    "        all_outputs['lif3_spikes'].append(spike3.detach().cpu().numpy())  # LIF3 spikes\n",
    "        all_outputs['mem3'].append(mem3.detach().cpu().numpy())\n",
    "\n",
    "        # Continue for other layers as necessary\n",
    "\n",
    "    return all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved model from best_SNN_model.pth\n",
      "\n",
      "torch.Size([31, 128, 2, 34, 34])\n",
      "(31, 2, 34, 34)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Model initialization\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SNN_tests(config).to(device)\n",
    "\n",
    "def save_single_sample_and_label_to_csv(test_loader, sample_file_name=\"sample.csv\", label_file_name=\"label.csv\",\n",
    "                                        model_path=\"best_SNN_model.pth\"):\n",
    "    \n",
    "    if os.path.isfile(model_path):\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(f\"Loaded saved model from {model_path}\\n\")\n",
    "    \n",
    "    # Extract the first sample and its label from the DataLoader\n",
    "    for data, targets in test_loader:\n",
    "        # Extract the first sample and label\n",
    "        print(data.shape)\n",
    "        first_sample = data[:, 0, :, :, :].numpy()  # Adjust this based on your actual data shape\n",
    "        print(first_sample.shape)\n",
    "        first_label = targets[0].numpy()\n",
    "\n",
    "        # Flatten the sample for saving to CSV\n",
    "        flattened_sample = first_sample.flatten()\n",
    "        \n",
    "        # Save the first sample to a CSV file\n",
    "        df_sample = pd.DataFrame(flattened_sample.reshape(1, -1))\n",
    "        df_sample.to_csv(sample_file_name, index=False, header=False)\n",
    "        \n",
    "        # Save the first label to a CSV file\n",
    "        df_label = pd.DataFrame([first_label])\n",
    "        df_label.to_csv(label_file_name, index=False, header=False)\n",
    "        \n",
    "        break  # Stop after the first sample\n",
    "    \n",
    "    return first_sample\n",
    "\n",
    "# Assuming test_loader is defined\n",
    "first_sample = save_single_sample_and_label_to_csv(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved model from best_SNN_model.pth\n",
      "\n",
      "torch.Size([31, 128, 2, 34, 34])\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n",
      "(2, 34, 34)\n"
     ]
    }
   ],
   "source": [
    "def save_single_sample_and_label_to_csv(test_loader, model_path=\"best_SNN_model.pth\"):\n",
    "    if os.path.isfile(model_path):\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print(f\"Loaded saved model from {model_path}\\n\")\n",
    "    \n",
    "    # Extract the first batch from the DataLoader\n",
    "    for data, targets in test_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        print(data.shape)\n",
    "        # Assuming the shape of 'data' is [batch_size, time_steps, channels, height, width]\n",
    "        # Adjust according to your dataset if needed\n",
    "        for timestep in range(data.shape[0]):\n",
    "            # Extract the first sample at the current timestep\n",
    "            first_sample = data[timestep, 0, :, :, :].cpu().numpy()  # [channels, height, width]\n",
    "            print(first_sample.shape)\n",
    "            \n",
    "            # Flatten the sample for saving to CSV and include the timestep index in the file name\n",
    "            flattened_sample = first_sample.flatten()\n",
    "            sample_file_name = f\"sample_timestep_{timestep}.csv\"\n",
    "            df_sample = pd.DataFrame(flattened_sample.reshape(1, -1))\n",
    "            df_sample.to_csv(sample_file_name, index=False, header=False)\n",
    "\n",
    "        # Assuming the label is a single value, save the label of the first sample\n",
    "        first_label = targets[0].cpu().numpy()\n",
    "        label_file_name = \"label.csv\"\n",
    "        df_label = pd.DataFrame([first_label])\n",
    "        df_label.to_csv(label_file_name, index=False, header=False)\n",
    "        \n",
    "        break  # Stop after processing the first batch/sample\n",
    "\n",
    "# Assuming test_loader is defined\n",
    "save_single_sample_and_label_to_csv(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 2, 34, 34)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n\u001b[1;32m     15\u001b[0m single_sample, single_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader))  \u001b[38;5;66;03m# Assuming test_loader is defined\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43msave_layer_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 9\u001b[0m, in \u001b[0;36msave_layer_outputs\u001b[0;34m(model, single_sample, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_name, output \u001b[38;5;129;01min\u001b[39;00m layer_outputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, step_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(layer_outputs):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# Convert to DataFrame and save\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[43mstep_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m     10\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(step_output)\n\u001b[1;32m     11\u001b[0m         df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_timestep_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def save_layer_outputs(model, single_sample, device):\n",
    "    single_sample = single_sample.to(device).unsqueeze(0)  # Add batch dimension\n",
    "    layer_outputs = model(first_sample)\n",
    "\n",
    "    # Save each layer's output to a separate CSV file\n",
    "    for layer_name, output in layer_outputs.items():\n",
    "        for step, step_output in enumerate(layer_outputs):\n",
    "            # Convert to DataFrame and save\n",
    "            #print(step_output.shape)\n",
    "            df = pd.DataFrame(step_output)\n",
    "            df.to_csv(f\"{layer_name}_timestep_{step}.csv\", index=False)\n",
    "\n",
    "# Example usage:\n",
    "model.eval()  # Set model to evaluation mode\n",
    "single_sample, single_label = next(iter(test_loader))  # Assuming test_loader is defined\n",
    "save_layer_outputs(model, single_sample, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 2, 34, 34)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "def save_layer_outputs(model, single_sample, device):\n",
    "    # Ensure single_sample is properly shaped\n",
    "    single_sample = single_sample.unsqueeze(0) if len(single_sample.shape) < 5 else single_sample\n",
    "    single_sample = single_sample.to(device)\n",
    "    \n",
    "    # Get the model's outputs\n",
    "    layer_outputs = model(first_sample)  # Assuming this returns a dict of layer outputs\n",
    "    \n",
    "    # Iterate through each layer's outputs\n",
    "    for layer_name, outputs in layer_outputs.items():\n",
    "        # Check if outputs is a list of timesteps; if not, wrap it in a list\n",
    "        if not isinstance(outputs, list):\n",
    "            outputs = [outputs]\n",
    "        \n",
    "        for step, step_output in enumerate(outputs):\n",
    "            # Assuming step_output is a tensor, convert it to numpy array\n",
    "            print(type(step_output))\n",
    "            step_output_np = step_output\n",
    "\n",
    "            # Reshape the output if necessary to match your expected format\n",
    "            # Flatten or adjust dimensions as needed for saving\n",
    "            # For example, if step_output_np is 4D (batch_size, channels, H, W) and you want to flatten it:\n",
    "            step_output_flat = step_output_np.flatten()\n",
    "\n",
    "            # Convert to DataFrame and save\n",
    "            df = pd.DataFrame(step_output_flat.reshape(1, -1))  # Reshape as 2D data\n",
    "            df.to_csv(f\"{layer_name}_timestep_{step}.csv\", index=False)\n",
    "\n",
    "# Assuming the model and device are defined, and model's forward() is implemented to return layer outputs\n",
    "model.eval()  # Set model to evaluation mode\n",
    "single_sample, _ = next(iter(test_loader))  # Grab a single sample\n",
    "save_layer_outputs(model, single_sample, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNNCpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
