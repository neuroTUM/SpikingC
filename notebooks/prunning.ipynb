{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import snntorch as snn\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import os\n",
    "#os.chdir('../') # Change the working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([31, 128, 2, 34, 34])\n",
      "Targets shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as torchvision_transforms\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "\n",
    "# Define sensor size for NMNIST dataset\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "\n",
    "# Define transformations\n",
    "# Note: The use of torch.from_numpy is removed as Tonic's transforms handle conversion.\n",
    "transform = tonic.transforms.Compose([\n",
    "    transforms.Denoise(filter_time=10000),\n",
    "    transforms.ToFrame(sensor_size=sensor_size, time_window=10000),\n",
    "    # torchvision.transforms.RandomRotation is not directly applicable to event data.\n",
    "    # If rotation is needed, it should be done on the frames after conversion by ToFrame.\n",
    "])\n",
    "\n",
    "# Load NMNIST datasets without caching\n",
    "trainset = tonic.datasets.NMNIST(save_to='tmp/data', transform=transform, train=True)\n",
    "testset = tonic.datasets.NMNIST(save_to='tmp/data', transform=transform, train=False)\n",
    "\n",
    "# Split trainset into training and validation datasets\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training, validation, and testing\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "\n",
    "# Fetch a single batch from the train_loader to inspect the shape\n",
    "data, targets = next(iter(train_loader))\n",
    "print(f\"Data shape: {data.shape}\")  # Example output: torch.Size([batch_size, timesteps, channels, height, width])\n",
    "print(f\"Targets shape: {targets.shape}\")  # Example output: torch.Size([batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # SNN\n",
    "    \"threshold1\": 2.5,\n",
    "    \"threshold2\": 8.0,\n",
    "    \"threshold3\": 4.0,\n",
    "    \"beta\": 0.5,\n",
    "    \"num_steps\": 10,\n",
    "    \n",
    "    # SNN Dense Shape\n",
    "    \"dense1_input\": 2312,\n",
    "    \"num_classes\": 10,\n",
    "\n",
    "    # Hyper Params\n",
    "    \"lr\": 0.007,\n",
    "\n",
    "    # Early Stopping\n",
    "    \"min_delta\": 1e-6,\n",
    "    \"patience_es\": 20,\n",
    "\n",
    "    # Training\n",
    "    \"epochs\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super(SNN, self).__init__()\n",
    "\n",
    "    # Initialize configuration parameters\n",
    "      # LIF\n",
    "    self.thresh1 = config[\"threshold1\"]\n",
    "    self.thresh2 = config[\"threshold2\"]\n",
    "    self.thresh3 = config[\"threshold3\"]\n",
    "    self.beta = config[\"beta\"]\n",
    "    self.num_steps = config[\"num_steps\"]\n",
    "\n",
    "      # Dense Shape\n",
    "    self.dense1_input = config[\"dense1_input\"]\n",
    "    self.num_classes = config[\"num_classes\"]\n",
    "\n",
    "      # Network Layers\n",
    "    self.fc1 = nn.Linear(self.dense1_input, self.dense1_input//4)\n",
    "    self.lif1 = snn.Leaky(beta=self.beta, threshold=self.thresh1)\n",
    "    \n",
    "    \n",
    "    self.fc2 = nn.Linear(self.dense1_input//4, self.dense1_input//8)\n",
    "    self.lif2 = snn.Leaky(beta=self.beta, threshold=self.thresh2)\n",
    "    \n",
    "    self.fc3 = nn.Linear(self.dense1_input//8, self.num_classes)\n",
    "    self.lif3 = snn.Leaky(beta=self.beta, threshold=self.thresh3)\n",
    "    \n",
    "    self.flatten = nn.Flatten()\n",
    "    \n",
    "    \n",
    "    # Forward Pass\n",
    "  def forward(self, inpt):\n",
    "    mem1 = self.lif1.init_leaky()\n",
    "    mem2 = self.lif2.init_leaky()\n",
    "    mem3 = self.lif3.init_leaky()\n",
    "\n",
    "    spike3_rec = []\n",
    "    mem3_rec = []\n",
    "\n",
    "    for step in range(inpt.shape[0]):\n",
    "      #print(inpt[step].shape)\n",
    "      \n",
    "      current_input = inpt[step]\n",
    "      current_input = self.flatten(current_input)\n",
    "      \n",
    "      current1 = self.fc1(current_input)\n",
    "      spike1, mem1 = self.lif1(current1, mem1)\n",
    "\n",
    "      current2 = self.fc2(spike1)\n",
    "      spike2, mem2 = self.lif2(current2, mem2)\n",
    "\n",
    "      current3 = self.fc3(spike2)\n",
    "      spike3, mem3 = self.lif3(current3, mem3)\n",
    "\n",
    "      spike3_rec.append(spike3)\n",
    "      mem3_rec.append(mem3)\n",
    "\n",
    "    return torch.stack(spike3_rec, dim=0), torch.stack(mem3_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SNN(\n",
       "  (fc1): Linear(in_features=2312, out_features=578, bias=True)\n",
       "  (lif1): Leaky()\n",
       "  (fc2): Linear(in_features=578, out_features=289, bias=True)\n",
       "  (lif2): Leaky()\n",
       "  (fc3): Linear(in_features=289, out_features=10, bias=True)\n",
       "  (lif3): Leaky()\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'model.pth'\n",
    "model = SNN(config)\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pruning to the model's layers\n",
    "for module_name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.3)\n",
    "        if module.bias is not None:\n",
    "            prune.l1_unstructured(module, name='bias', amount=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in fc1: 30.0000149662959%\n",
      "Sparsity in fc2: 30.000239460734427%\n",
      "Sparsity in fc3: 30.0%\n"
     ]
    }
   ],
   "source": [
    "# Inspect pruned model and make pruning permanent\n",
    "for module_name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        print(f\"Sparsity in {module_name}: {100. * float(torch.sum(module.weight == 0)) / float(module.weight.nelement())}%\")\n",
    "        prune.remove(module, 'weight')\n",
    "        if module.bias is not None:\n",
    "            prune.remove(module, 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, device, model_path=\"best_SNN_model.pth\"):\n",
    "\n",
    "    # Initialize variables for test loss and accuracy\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    # Switch model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over the test data\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, _ = model(data)  # Modify according to your model's output\n",
    "            outputs = outputs.mean(dim=0)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += targets.size(0)\n",
    "            correct_test += (predicted == targets).sum().item()\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.0592, Test Accuracy: 94.14%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cpu\")\n",
    "test_loss, test_accuracy = test(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNNCpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
